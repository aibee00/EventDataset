/* Proto definition for tracking results. */

syntax = "proto3";

package tracking_pipeline.offline;

import "common.proto";
import "version.proto";

message TrackingResults {
  string detection_file = 1;
  string video_path = 2;
  float detection_confidence = 3;
  repeated TrackingDetection tracked_detections = 4;
  string version = 5;
  repeated Tracklet tracklets = 6;              // store per-tracklet information
  string face_feature_version = 7;              // version of face feature
  ModelVersions model_versions = 8;
  ModuleVersions module_versions = 9;
}

message Tracklet {
  int32 tracklet_index = 1;                     // should match with tracklet index in each TrackingDetection
  repeated int32 pid_labels = 2;                // suggested PIDs (sorted based on the probabilities)
  repeated float pid_probabilities = 3;         // corresponding probabilites of labels. (sum should be less than 1, sum up to 1 if all labels are provided).
  repeated int32 merge_suggestions = 4;         // other tracklet IDs with potential merges
  repeated float merge_probabilities = 5;       // corresponding probabilities. This information can be combined with following PID suggestions to update PID suggestion on the fly. (0 ~ 1)
  repeated int32 split_frame_index = 6;         // suggested split position to eliminate impurity
  repeated float split_probability = 7;         // corresponding split probability (0 ~ 1)
  repeated float appearance_feature = 8;        // representative appearance feature (if storage is an issue, we can try compressing before saving using PCA or etc).
  repeated float face_feature = 9;              // representative face features.
  repeated int32 visually_discriminative_frames = 10; // good frames to show for a snapshot. e.g, face visible, less occluded, etc.
  repeated float visually_discriminative_scores = 11;  // associate score of the frames

  // frame index of the best face feature
  int32 best_face_feature_frame_index = 12;

  // pid assignment from the CRF (marginal probability over certain threshold, e.g. 0.99)
  int32 assigned_pid = 13;

  // face quality of the selected face feature
  float best_face_quality = 14;

  // in case the face feature are linked to singleview tracklet ids
  int32 sv_tracklet_id = 15;

  // Whether the sv_tracklet_id got split by labeler.
  // Note that this value always corresponds to sv_tracklet_id (tag 15).
  bool is_split_by_labeler = 16;

  // mark valid tracks here (static false positives, out of the store area, etc)
  bool is_invalid = 17;

  // long track id : after inference group sv tracks by spatio-temporal continuity
  string long_track_id = 18;

  int32 face_recog_id = 19;

  bool is_sv_face_recog = 20;

  string assigned_str_pid = 21;
  // Next tag: 22
}

message StaffRecognitionResult {
  // default value "0" means not-found/no-match from staff database.
  int32 staff_recog_id = 1;

  // corresponding face_id.
  int32 face_id = 2;

  float confidence = 3;
}

message CustomerRecognitionResult {
  // default value "0" means not-found/no-match from customer database.
  int32 customer_recog_id = 1;

  float confidence = 2;
}

message PIDClassificationResult {
  // default value "0" means not-found / no-match.
  int32 pid = 1;
  float confidence = 2; // dummy for now
}

message LabelingResult {
  int32 pure_tracklet_id = 1; // tracklet id of single view results after purity check
  int32 long_tracklet_id = 2; // long tracklet id of merged long tracks
  int32 pid = 3;             // pid of associated long tracks
  string str_pid = 4;        // pid in string format
  string id_type = 5;        // staff, customer, etc.
}

message Localization {
    float x = 1;
    float y = 2;
    uint64 det_time_msec = 3;
}

enum DetectionPostureType {
  DETECTION_POSTURE_UNKNOWN = 0;
  DETECTION_POSTURE_STANDING = 1;
  DETECTION_POSTURE_SEATED = 2;
}

enum DetectionViewType {
  DETECTION_VIEW_UNKNOWN = 0;
  DETECTION_VIEW_FRONT = 1;
  DETECTION_VIEW_REAR = 2;
  DETECTION_VIEW_SIDE = 3;
}

// Tracking Detection contains the information for one detection (either face, head, or body)
// and their related track IDs
// Although different targets, e.g., human, head, faces, are described, only one of them is used.
message TrackingDetection {
  string video_name = 20;

  uint32 frame_index = 1;
  uint32 tracklet_index = 2;
  bool track_confirmed = 3;

  bool body_visible = 4;
  repeated KeyPoint key_points = 5;
  repeated float body_kf_velocity = 6;

  float human_detection_score = 7;
  int32 human_box_x = 8;
  int32 human_box_y = 9;
  int32 human_box_height = 10;
  int32 human_box_width = 11;

  int32 head_box_x = 12;
  int32 head_box_y = 13;
  int32 head_box_height = 14;
  int32 head_box_width = 15;

  // Saving apperance_feature is usually memory/storage inefficient.
  bytes appearance_feature = 16;

  // This is the originial single view tracklet id.
  // The difference between tracklet_id and sv_tracklet_id is that, after mv and/or final post processing,
  // tracklet_id might be updated by various association/merging strategies. But sv_tracklet_id will stay
  // the same as the original single_view tracklets.
  int32 sv_tracklet_id = 23;

  /* The following parameters are only present after further processing on single view tracking results - for example,
   multi-view association, face_track/human_track matching, or staff recognition.
  */

  // "linked_id" after MV associations.
  int32 mv_linked_id = 17;

  // Face recognition tries to map tracklet to recognized faces in database.
  // deafult value "0" (proto3 stops supporting explicit default values) means not-found/no-match from face database.
  int32 face_recog_id = 18;

  // Staff recognition tries to map tracklet to recognized staff members.
  StaffRecognitionResult staff_recognition_result = 19;
  // Labeling results.
  LabelingResult labeling_result = 21;

  // Customer recognition tries to map tracklet to recognized customers.
  CustomerRecognitionResult customer_recognition_result = 22;
  
  // PID classification result.
  PIDClassificationResult pid_classification_result = 24;

  // DID to FID/fisheye via DL mapping
  int32 did_mapped_id = 34;

  // face boxes
  int32 face_box_x = 25;
  int32 face_box_y = 26;
  int32 face_box_height = 27;
  int32 face_box_width = 28;
  float face_confidence_score = 29;
  float face_quality_score = 30;
  float face_fid_did_feature_distance = 31;
  Box_UUID human_box_uuid = 32;

  // track id for the head-body merge and pano-bot merge steps, still part of singleview process
  int32 sv_head_body_merged_id = 35;
  int32 sv_pano_bot_merged_id = 36;

  // deprecated sit_stand field
  int32 sit_stand = 37 [deprecated=true]; 
  // localization
  Localization loc = 38;
  // sit stand indicator
  DetectionPostureType detection_posture_type = 39;

  DetectionViewType detection_view_type = 40;
  float head_angle = 42;
  int32 track_id = 43 ;
  // Next tag: 44
}
